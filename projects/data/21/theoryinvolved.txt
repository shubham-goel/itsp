<ul>
<li>Our Approach for voice recognition</li></ul><p><strong>Hidden Markov Model using Cepstral coefficients </strong> : The following steps were followed for voice recognition using Hidden Markov Model Obtaining the signal The first part is obtaining signal from microphone. This is easily achieved by using the audiorecorder object of MATLAB. The getaudiodata function of audiorecorder object gives the signal data in form of column of numbers representing the audio signal. Processing the signal into observation Then comes the feature analysis part of processing.</p><p><strong>Filtering / pre-emphasis</strong><br>First the signal is passed through a part of code which acts as a high pass filter known as fir filter . This spectrally flattens the signal. <br><strong>Voice Activation Detection</strong><br>The signal is now passed to a function which finds the part of signal containing the spoken word and thus returns the signal without silence. Now for the word obtained we proceed further. Now onwards Signal would refer to the extracted words. <br><strong>Finding cepstral coefficients and delta coefficients</strong><br>Using some predefined functions of MATLAB we convert the given signal into a set of observations which can be used for the model. The various operations that are performed are breaking the signal into windows , then applying the window function for each (hamming window is used) , then autocorelation coefficients are obtained, from these coefficients we obtain the cepstral coefficients by levinson durbin algorithm , further these coefficients are weighted to normalize , and the last step involves in finding the delta cepstral coefficients. Thus we get two set of coefficients which are merged to form row of 24 elements for each window and this completes the conversion into observation sequence. <br><strong>Choice of model parameters</strong><br>The number of states we selected was 6 after analysis of number of states versus the error made by the system / model. The data being obtained from paper by Rabiner over speech recognition by HMM model. <br>Next the HMM model to be used is selected to be Left-to-Right as the speech signals have parts which follow one after the other to form a meaningful word. Left-to-Right model has property that the system can move into the same state or a state ahead of it and not backward. The other type being ergodic model where system can move from any state to any other. Training the system/model Training the system here means determining the model parameters so that the given observation sequence used to train the model is declared highly probable by the system and others low. <br><strong>Parameters</strong><br>The parameters for HMM model are transition probabilities between the states represented by matrix A (whose elements at ith row , jth column represent transition probability form ith state to jth), then comes the probabilities of occurrence of each observation in a particular word represented by B_ik meaning probability of kth observation being in ith state , then are the initial probabilities for occurrence of a given state at beginning time. In our case since we have dealt with continuous data the B_ik is represented as a gaussian distribution. <br><strong>Initial estimation of the parameters</strong><br>For better convergence of the estimation by K-Means algorithm we need to have good initial estimation of the parameters. The transition probabilities are made equal for the states into which the system may go. Rest are made zero. The mean and variance for the gaussian distribution are calculated from the 5 data sets obtained. The pi values for each state are set to be equally likely.</p><pre>&lt;strong&gt;Readjusting the model parameters&lt;/strong&gt;&lt;br&gt;The method used for performing this is based on classic work of BAUM and his colleagues . For this various variables are defined as follows-- &lt;br&gt;alpha(t,i)--- (forward variable) The probability of partial observation O1,O2,O3.... Ot (until time t) and ith state at time t, given the model .&lt;br&gt;beta(t,i)--- (backward variable) The probability of partial observation O(t+1) till end with ith state at time t. &lt;br&gt;gamma(t,i)--- The probability of observation sequence given the model with state ith at time t. &lt;br&gt;eta(t,i,j)--- The probability of observation sequence given the model with state ith at time t and jth at time t+1. &lt;br&gt;delta(t,i)--- The best score(highest probability) along a single path , at time t , which accounts for first t observations and end in state ith. &lt;br&gt;Now we have following adjustments-- &lt;br&gt;pi(i) = expected frequency (number of times) in state ith at time (t=1)= gamma(1,i) &lt;br&gt;A(i,j) = (expected number of transitions from state ith to jth)/(expected number of transitions from state ith) = (summation over t from 1 to T-1 (eta(t,i,j)))/(summation over t from 1 to T-1 (gamma(t,i))) &lt;br&gt;mean(i)= (summation over t from 1 to T (gamma(t,i)*O(t))) /(summation over t form 1 to T (gamma(t,i))) &lt;br&gt;variance(i)=(summation over t from 1 to T (gamma(t,i)*(O(t)-mean(i))*(transpose(O(t)-mean(i)))))/(summation over 1 to T (gamma(t,i))).</pre><p><br>Thus the new model parameters are obtained and the HMM model for the word is obtained.</p><p><strong>Identifying the word</strong><br>The probabilities for a given sequence is obtained with the different HMMs and highest among those is declared as the required word (probability must be greater than a threshold). <br><strong>Obtaining the probability</strong><br>The algorithm used to find the probability is viterbi algorithm. It finds the highest probability among the best state sequence for the given observation sequence.</p><p>Note : We have also attached some of the documents which have the detailed descriptions of the algorithms which we have used in our system.</p><ul>
<li>To get the description of the bots please refer the documentation which is present along with codes</li></ul>